{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4a27ee-a63c-40a9-b8e9-8539635897fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6845e079-99aa-4b13-b9a0-b3ae6ba860f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c58eb9-4bff-436e-9e9e-cbbbbcd11011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile, Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047075ae-76a7-4542-8a57-ee4d26841689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 4\n",
      "GPU 0: Tesla V100-SXM2-16GB\n",
      "GPU 1: Tesla V100-SXM2-16GB\n",
      "GPU 2: Tesla V100-SXM2-16GB\n",
      "GPU 3: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No GPUs detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5a094b-6aa5-4b9d-b341-0db5c64a1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b545b37-cd77-4d27-a6d5-4c47789d1916",
   "metadata": {},
   "source": [
    "List of 9 image names to be removed; 6 of these have a negative PCI. Additionally, the JPEG files associated with '1119_' and '11980_' also had issues.\n",
    "\n",
    "\n",
    "image_names_to_remove = [\n",
    "    \"1119_[40.703396736153195, -89.40525218403829]_ 2023-10-18 15-49-01_14.30605_LD.jpg\",\n",
    "    \"6522_(40.7031446666667, -89.4013051666667)_ 2023-10-18 16-53-57_14.2894_D.jpg\",\n",
    "    \"6706_(40.7035503333333, -89.4043398333333)_ 2023-10-18 16-59-31_0.09805_D.jpg\",\n",
    "    \"1119_(40.7034233333333, -89.4052588333333)_ 2023-10-18 15-49-01_14.30605_D.jpg\",\n",
    "    \"6523_(40.7031381666667, -89.4013561666667)_ 2023-10-18 16-53-58_14.73525_D.jpg\",\n",
    "    \"1287_[40.699373956663266, -89.40239271668378]_ 2023-10-20 21-38-40_18.9699_LD.jpg\",\n",
    "    \"11980_(40.6916493333333, -89.4393206666667)_ 2023-10-20 16-10-54_28.5085_D.jpg\",\n",
    "    \"8536_(40.703158, -89.4063703333333)_ 2023-10-18 18-05-12_21.645_D.jpg\",\n",
    "    \"6760_(40.7028968333333, -89.4042638333333)_ 2023-10-18 17-01-43_7.74595_D.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309571ca-603f-4ad0-99aa-b0189e91e1de",
   "metadata": {},
   "source": [
    "# An important note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b953e-d9ad-469c-99e0-7befc07ef28d",
   "metadata": {},
   "source": [
    "I have removed those 9 rows from the train.csv file that was provided in training dataset and also performed sorting to better organize the data for my analysis. Since we apply shuffling during training and aim for reproducible results, and the two CSV files are not the same, I provide the link to the CSV file that I am using. Please download it and use this file instead of the train.csv in the dataset. In my code, I have named this filtered file as filtered_train to emphasize the difference from the initial train.csv file, with 7695 rows and sorted, in contrast to the original train.csv with 7704 rows and not sorted.\n",
    "\n",
    "https://drive.google.com/file/d/1pZPIqic56nF6CVytcucb6NvYdn63144n/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae417741-45fa-43e6-80b5-94f2bb9abe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(14) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb37df17-b196-4265-b2cb-df09463a851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group\n",
      "(0, 5]        314\n",
      "(5, 10]        91\n",
      "(10, 15]       89\n",
      "(15, 20]      219\n",
      "(20, 25]      240\n",
      "(25, 30]      181\n",
      "(30, 35]      168\n",
      "(35, 40]      455\n",
      "(40, 45]      625\n",
      "(45, 50]      568\n",
      "(50, 55]       78\n",
      "(55, 60]       89\n",
      "(60, 65]      964\n",
      "(65, 70]       94\n",
      "(70, 75]      175\n",
      "(75, 80]      289\n",
      "(80, 85]      116\n",
      "(85, 90]      282\n",
      "(90, 95]      279\n",
      "(95, 100]    2367\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "bins = list(range(0,105 , 5))\n",
    "data['group'] = pd.cut(data.pci, bins, right=True)\n",
    "group_frequency = data['group'].value_counts().sort_index()\n",
    "\n",
    "print(group_frequency, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44455522-b5dd-43d5-a5da-ebd39601c505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['pci'], color='blue',  bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c34a87-db57-46c5-a961-447ffc307bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before applying oversampling for PCI values less than or equal to 30 (in the AsphaltDataset class), the general_transform was simpler.\n",
    "\n",
    "general_transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "aggressive_transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56e60e9-a891-43c9-81c5-691cf5393519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAnother experiment involves transforming the PCI value to be between -1 and 1:\\n\\n  def __getitem__(self, idx):\\n        actual_idx = self.indices[idx]\\n        img_name = os.path.join(self.root_dir, self.data_frame.iloc[actual_idx, 0])\\n        image = Image.open(img_name).convert('RGB')\\n        pci_value = float(self.data_frame.iloc[actual_idx, 1])\\n        \\n        # Apply the appropriate transformation based on PCI value\\n        if pci_value < 30:\\n            image = aggressive_transform(image)\\n        else:\\n            image = general_transform(image)\\n        \\n        # Transform PCI value to be between -1 and 1\\n        transformed_pci = (pci_value / 50) - 1\\n        pci_value = torch.tensor(transformed_pci, dtype=torch.float32)\\n        \\n        return image, pci_value\\n\\n\\nAnother experiment involves adding a slight amount of noise to PCI values, particularly for those values associated with a large number of images (such as a PCI of 64 or 100):\\n\\ndef __getitem__(self, idx):\\n        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\\n        image = Image.open(img_name).convert('RGB')\\n        pci_value = self.data_frame.iloc[idx, 1]  # Assuming PCI value is in the second column\\n        pci_value = float(pci_value)\\n        \\n        # Add Gaussian noise to the PCI value\\n        noise = np.random.normal(0.5, 0.2)\\n        pci_value += noise\\n\\n        pci_value = torch.tensor(pci_value, dtype=torch.float32)\\n\\n        if self.transform:\\n            image = self.transform(image)\\n\\n        return image, pci_value\\n        \\n*** The two experiments did not yield meaningful improvements, so I proceeded with the augmentation and oversampling already implemented in the AsphaltDataset class.        \\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AsphaltDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Identifying indices with PCI < 30 and not equal to 4, 16, or 25\n",
    "        excluded_pcis = []\n",
    "        self.low_pci_indices = self.data_frame[(self.data_frame['pci'] < 35) & (~self.data_frame['pci'].isin(excluded_pcis))].index.tolist()\n",
    "        \n",
    "        self.indices = list(range(len(self.data_frame))) + self.low_pci_indices * 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Adjust to use self.indices for fetching data\n",
    "        actual_idx = self.indices[idx]\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[actual_idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        pci_value = float(self.data_frame.iloc[actual_idx, 1])\n",
    "\n",
    "        # Apply the appropriate transformation based on PCI value\n",
    "        if pci_value < 30:\n",
    "            image = aggressive_transform(image)\n",
    "        else:\n",
    "            image = general_transform(image)\n",
    "\n",
    "        # Transform PCI value if necessary\n",
    "        pci_value = torch.tensor(pci_value, dtype=torch.float32)\n",
    "        return image, pci_value\n",
    "    \n",
    "    \n",
    "'''\n",
    "Another experiment involves transforming the PCI value to be between -1 and 1:\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[actual_idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        pci_value = float(self.data_frame.iloc[actual_idx, 1])\n",
    "        \n",
    "        # Apply the appropriate transformation based on PCI value\n",
    "        if pci_value < 30:\n",
    "            image = aggressive_transform(image)\n",
    "        else:\n",
    "            image = general_transform(image)\n",
    "        \n",
    "        # Transform PCI value to be between -1 and 1\n",
    "        transformed_pci = (pci_value / 50) - 1\n",
    "        pci_value = torch.tensor(transformed_pci, dtype=torch.float32)\n",
    "        \n",
    "        return image, pci_value\n",
    "\n",
    "\n",
    "Another experiment involves adding a slight amount of noise to PCI values, particularly for those values associated with a large number of images (such as a PCI of 64 or 100):\n",
    "\n",
    "def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        pci_value = self.data_frame.iloc[idx, 1]  # Assuming PCI value is in the second column\n",
    "        pci_value = float(pci_value)\n",
    "        \n",
    "        # Add Gaussian noise to the PCI value\n",
    "        noise = np.random.normal(0.5, 0.2)\n",
    "        pci_value += noise\n",
    "\n",
    "        pci_value = torch.tensor(pci_value, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, pci_value\n",
    "        \n",
    "*** The two experiments did not yield meaningful improvements, so I proceeded with the augmentation and oversampling already implemented in the AsphaltDataset class.        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b308950a-306d-4f8e-b9e5-e2f67e8da7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class for Inference on Testing Data\n",
    "class AsphaltInferenceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=test_transform):  # Set default to general_transform\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_names = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a30a64e-167b-41aa-9882-0447ca9c2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure reproducibility, the new CSV file (filtered_train), from which 9 rows have been deleted and sorting has been applied, should be used instead of the initial train.csv.\n",
    "\n",
    "train_dataset = AsphaltDataset(csv_file='filtered_train.csv', root_dir='Training_images')\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = AsphaltInferenceDataset(root_dir='Testing_images', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b64f031-c838-4591-87c9-418ff015b2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training set after oversampling: 10251\n",
      "Number of images in the testing set: 3302\n"
     ]
    }
   ],
   "source": [
    "num_train_images = len(train_dataset)\n",
    "num_test_images = len(test_dataset)\n",
    "\n",
    "print(\"Number of images in the training set after oversampling:\", num_train_images)\n",
    "print(\"Number of images in the testing set:\", num_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e84796-473d-4cec-9620-a7e67ef2bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch-base_1690385890458/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# For the feature extraction phase, two different transformers and three CNNs were tested, and I found that swinv2_large performed the best.\n",
    "# For the fully connected (fc) layer:\n",
    "# (i) two models were tested: 1536-512-512-256-1 (final) and 1536-512-256-1.\n",
    "# (ii) XGBoost  was tested both with and without PCI.\n",
    "\n",
    "\n",
    "# Model Definition with Fine-Tuning Option\n",
    "class AsphaltNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(AsphaltNet, self).__init__()\n",
    "        self.feature_extractor = timm.create_model('swinv2_large_window12to24_192to384', pretrained=True, num_classes=0)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1536, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.regressor(features)\n",
    "        return output\n",
    "    \n",
    "model = AsphaltNet()\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14512fab-e4f5-4144-8a06-a42c67385613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_hms(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\"\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=50, save_path='swinv2large'):\n",
    "    model.train()\n",
    "    models_dir = 'rep4'\n",
    "    os.makedirs(models_dir, exist_ok=True)  # Creates the directory if it doesn't exist\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        all_pcis = []\n",
    "        all_outputs = []\n",
    "        \n",
    "        for images, pcis in train_loader:\n",
    "            images, pcis = images.to(device), pcis.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), pcis)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Collect all outputs and pcis for metrics calculation\n",
    "            all_pcis.extend(pcis.detach().cpu().numpy())\n",
    "            all_outputs.extend(outputs.detach().squeeze().cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_time = time.time() - start_time  # Calculate cumulative runtime\n",
    "\n",
    "        # Convert lists to numpy arrays for calculation\n",
    "        all_pcis = np.array(all_pcis)\n",
    "        all_outputs = np.array(all_outputs)\n",
    "\n",
    "        # Calculate R^2 manually\n",
    "        ss_res = np.sum((all_pcis - all_outputs) ** 2)\n",
    "        ss_tot = np.sum((all_pcis - np.mean(all_pcis)) ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "        # Convert epoch_time to hh:mm:ss\n",
    "        formatted_time = seconds_to_hms(epoch_time)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, R^2: {r2:.4f}, Time: {formatted_time}\")\n",
    "\n",
    "        # Save model at each epoch\n",
    "        model_path = os.path.join(models_dir, f\"{save_path}_epoch{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model saved at epoch {epoch+1} with Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b6eefb5-1c64-453a-a2df-ab49d17ffdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Perform Inference on Testing Data\n",
    "def perform_inference(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c787a21-27f1-470f-aaf5-e7361a4eefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 1195.2982, R^2: -0.0152, Time: 00:10:38\n",
      "Model saved at epoch 1 with Training Loss: 1195.2982\n",
      "Epoch [2/50], Training Loss: 340.7748, R^2: 0.7106, Time: 00:21:11\n",
      "Model saved at epoch 2 with Training Loss: 340.7748\n",
      "Epoch [3/50], Training Loss: 261.8695, R^2: 0.7777, Time: 00:31:45\n",
      "Model saved at epoch 3 with Training Loss: 261.8695\n",
      "Epoch [4/50], Training Loss: 214.4035, R^2: 0.8185, Time: 00:42:15\n",
      "Model saved at epoch 4 with Training Loss: 214.4035\n",
      "Epoch [5/50], Training Loss: 184.7744, R^2: 0.8431, Time: 00:52:45\n",
      "Model saved at epoch 5 with Training Loss: 184.7744\n",
      "Epoch [6/50], Training Loss: 159.7408, R^2: 0.8644, Time: 01:03:16\n",
      "Model saved at epoch 6 with Training Loss: 159.7408\n",
      "Epoch [7/50], Training Loss: 145.7425, R^2: 0.8763, Time: 01:13:51\n",
      "Model saved at epoch 7 with Training Loss: 145.7425\n",
      "Epoch [8/50], Training Loss: 128.7808, R^2: 0.8907, Time: 01:24:25\n",
      "Model saved at epoch 8 with Training Loss: 128.7808\n",
      "Epoch [9/50], Training Loss: 119.9189, R^2: 0.8982, Time: 01:35:00\n",
      "Model saved at epoch 9 with Training Loss: 119.9189\n",
      "Epoch [10/50], Training Loss: 104.9545, R^2: 0.9109, Time: 01:45:34\n",
      "Model saved at epoch 10 with Training Loss: 104.9545\n",
      "Epoch [11/50], Training Loss: 100.5073, R^2: 0.9148, Time: 01:56:05\n",
      "Model saved at epoch 11 with Training Loss: 100.5073\n",
      "Epoch [12/50], Training Loss: 94.8108, R^2: 0.9195, Time: 02:06:36\n",
      "Model saved at epoch 12 with Training Loss: 94.8108\n",
      "Epoch [13/50], Training Loss: 89.8260, R^2: 0.9237, Time: 02:17:10\n",
      "Model saved at epoch 13 with Training Loss: 89.8260\n",
      "Epoch [14/50], Training Loss: 79.8930, R^2: 0.9322, Time: 02:27:43\n",
      "Model saved at epoch 14 with Training Loss: 79.8930\n",
      "Epoch [15/50], Training Loss: 81.4333, R^2: 0.9308, Time: 02:38:15\n",
      "Model saved at epoch 15 with Training Loss: 81.4333\n",
      "Epoch [16/50], Training Loss: 77.1356, R^2: 0.9345, Time: 02:48:48\n",
      "Model saved at epoch 16 with Training Loss: 77.1356\n",
      "Epoch [17/50], Training Loss: 72.2581, R^2: 0.9386, Time: 02:59:21\n",
      "Model saved at epoch 17 with Training Loss: 72.2581\n",
      "Epoch [18/50], Training Loss: 73.9747, R^2: 0.9372, Time: 03:09:53\n",
      "Model saved at epoch 18 with Training Loss: 73.9747\n",
      "Epoch [19/50], Training Loss: 62.4916, R^2: 0.9469, Time: 03:20:22\n",
      "Model saved at epoch 19 with Training Loss: 62.4916\n",
      "Epoch [20/50], Training Loss: 61.8868, R^2: 0.9475, Time: 03:30:53\n",
      "Model saved at epoch 20 with Training Loss: 61.8868\n",
      "Epoch [21/50], Training Loss: 63.4669, R^2: 0.9463, Time: 03:41:23\n",
      "Model saved at epoch 21 with Training Loss: 63.4669\n",
      "Epoch [22/50], Training Loss: 68.0306, R^2: 0.9422, Time: 03:51:52\n",
      "Model saved at epoch 22 with Training Loss: 68.0306\n",
      "Epoch [23/50], Training Loss: 67.3447, R^2: 0.9430, Time: 04:02:21\n",
      "Model saved at epoch 23 with Training Loss: 67.3447\n",
      "Epoch [24/50], Training Loss: 54.9889, R^2: 0.9533, Time: 04:12:54\n",
      "Model saved at epoch 24 with Training Loss: 54.9889\n",
      "Epoch [25/50], Training Loss: 56.3499, R^2: 0.9521, Time: 04:23:24\n",
      "Model saved at epoch 25 with Training Loss: 56.3499\n",
      "Epoch [26/50], Training Loss: 54.2991, R^2: 0.9539, Time: 04:33:55\n",
      "Model saved at epoch 26 with Training Loss: 54.2991\n",
      "Epoch [27/50], Training Loss: 61.0883, R^2: 0.9481, Time: 04:44:29\n",
      "Model saved at epoch 27 with Training Loss: 61.0883\n",
      "Epoch [28/50], Training Loss: 54.5725, R^2: 0.9536, Time: 04:55:00\n",
      "Model saved at epoch 28 with Training Loss: 54.5725\n",
      "Epoch [29/50], Training Loss: 54.2526, R^2: 0.9539, Time: 05:05:34\n",
      "Model saved at epoch 29 with Training Loss: 54.2526\n",
      "Epoch [30/50], Training Loss: 54.9440, R^2: 0.9533, Time: 05:16:04\n",
      "Model saved at epoch 30 with Training Loss: 54.9440\n",
      "Epoch [31/50], Training Loss: 49.2642, R^2: 0.9582, Time: 05:26:37\n",
      "Model saved at epoch 31 with Training Loss: 49.2642\n",
      "Epoch [32/50], Training Loss: 48.8832, R^2: 0.9585, Time: 05:37:10\n",
      "Model saved at epoch 32 with Training Loss: 48.8832\n",
      "Epoch [33/50], Training Loss: 50.9939, R^2: 0.9567, Time: 05:47:41\n",
      "Model saved at epoch 33 with Training Loss: 50.9939\n",
      "Epoch [34/50], Training Loss: 48.8679, R^2: 0.9585, Time: 05:58:11\n",
      "Model saved at epoch 34 with Training Loss: 48.8679\n",
      "Epoch [35/50], Training Loss: 47.8012, R^2: 0.9594, Time: 06:08:45\n",
      "Model saved at epoch 35 with Training Loss: 47.8012\n",
      "Epoch [36/50], Training Loss: 45.2794, R^2: 0.9615, Time: 06:19:18\n",
      "Model saved at epoch 36 with Training Loss: 45.2794\n",
      "Epoch [37/50], Training Loss: 46.4208, R^2: 0.9606, Time: 06:29:53\n",
      "Model saved at epoch 37 with Training Loss: 46.4208\n",
      "Epoch [38/50], Training Loss: 49.6511, R^2: 0.9578, Time: 06:40:26\n",
      "Model saved at epoch 38 with Training Loss: 49.6511\n",
      "Epoch [39/50], Training Loss: 49.9121, R^2: 0.9576, Time: 06:50:59\n",
      "Model saved at epoch 39 with Training Loss: 49.9121\n",
      "Epoch [40/50], Training Loss: 43.9295, R^2: 0.9627, Time: 07:01:30\n",
      "Model saved at epoch 40 with Training Loss: 43.9295\n",
      "Epoch [41/50], Training Loss: 46.7485, R^2: 0.9603, Time: 07:12:02\n",
      "Model saved at epoch 41 with Training Loss: 46.7485\n",
      "Epoch [42/50], Training Loss: 46.4320, R^2: 0.9606, Time: 07:22:34\n",
      "Model saved at epoch 42 with Training Loss: 46.4320\n",
      "Epoch [43/50], Training Loss: 51.1745, R^2: 0.9566, Time: 07:33:05\n",
      "Model saved at epoch 43 with Training Loss: 51.1745\n",
      "Epoch [44/50], Training Loss: 39.9507, R^2: 0.9661, Time: 07:43:35\n",
      "Model saved at epoch 44 with Training Loss: 39.9507\n",
      "Epoch [45/50], Training Loss: 37.5598, R^2: 0.9681, Time: 07:54:05\n",
      "Model saved at epoch 45 with Training Loss: 37.5598\n",
      "Epoch [46/50], Training Loss: 44.4707, R^2: 0.9623, Time: 08:04:38\n",
      "Model saved at epoch 46 with Training Loss: 44.4707\n",
      "Epoch [47/50], Training Loss: 43.0287, R^2: 0.9635, Time: 08:15:11\n",
      "Model saved at epoch 47 with Training Loss: 43.0287\n",
      "Epoch [48/50], Training Loss: 40.6151, R^2: 0.9656, Time: 08:25:49\n",
      "Model saved at epoch 48 with Training Loss: 40.6151\n",
      "Epoch [49/50], Training Loss: 41.5525, R^2: 0.9647, Time: 08:36:29\n",
      "Model saved at epoch 49 with Training Loss: 41.5525\n",
      "Epoch [50/50], Training Loss: 41.8856, R^2: 0.9644, Time: 08:47:06\n",
      "Model saved at epoch 50 with Training Loss: 41.8856\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=50, save_path='swinv2large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43de4e-0489-4af8-adf6-57d394e607dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348074f5-3df5-4a80-bc63-14015866b83e",
   "metadata": {},
   "source": [
    "| Epoch | MAPE  | MAE  | MAPE + MAE |\n",
    "|-------|-------|------|------------|\n",
    "| 42    | 33.784| 8.509| 42.293     |\n",
    "| 40    | 34.58 | 8.92 | 43.5       |\n",
    "| 38    | 34.37 | 9.21 | 43.58      |\n",
    "| 41    | 35.919| 8.053| 43.972     |\n",
    "\n",
    "Download the best model cooresponding to Epoch 42:\n",
    "https://drive.google.com/file/d/165058DYOueVK4NMP3wAbuYo8zIxCG32C/view?usp=sharing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It should also be noted that before setting a fixed seed value at the beginning of this code, I conducted another experiment in which, at Epoch 45, I obtained the following results:\n",
    "\n",
    "| Epoch | MAPE  | MAE  | MAPE + MAE |\n",
    "|-------|-------|------|------------|\n",
    "| 45    | 33.226| 8.322| 41.548     |\n",
    "\n",
    "Model:\n",
    "https://drive.google.com/file/d/1p55b3a6xLvb0mVyJsnrhV1tE6j_22iF1/view?usp=sharing\n",
    "\n",
    "json: https://drive.google.com/file/d/1gmbfSmNpAQmA5jOs0Vrs_hIkEz3LVgYB/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76f12d6b-283d-4fde-8496-f751b984d934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8084a356-62f3-493c-a042-6e592c44e9d2",
   "metadata": {},
   "source": [
    "I also decreased the learning rate to 0.000001 after 50 epochs and trained for 15 more epochs, but I didn't observe any improvement, so I removed that part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7c33e-ecc3-4328-82e9-35cd373e721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "output_dir = 'rep4'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    \n",
    "def gen_submit(df, epoch, output_dir):\n",
    "    out_json = []\n",
    "    for idx, results in df.iterrows():\n",
    "        out_json.append({results['image_name']: results['PCI']})\n",
    "    json_file_path = os.path.join(output_dir, f'swinv2large_epoch{epoch}.json')\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump(out_json, f)\n",
    "\n",
    "def process_csv_to_json(csv_file_path, epoch, output_dir):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    gen_submit(df, epoch, output_dir)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    model_file_name = f'replicate/swinv2large_epoch{epoch}.pth'\n",
    "    output_csv_file = os.path.join(output_dir, f'swinv2large_epoch{epoch}.csv')\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_file_name, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    test_dataset = AsphaltInferenceDataset(root_dir='Testing_images', transform=test_transform)\n",
    "    test_dataset.img_names = sorted(test_dataset.img_names, key=lambda x: int(os.path.basename(x).split('_')[0]))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    predictions = perform_inference(model, test_loader, device)\n",
    "    flattened_predictions = [pred[0] for pred in predictions]  # Adjust based on the structure of `predictions`\n",
    "    test_file_names = [os.path.basename(fname) for fname in test_dataset.img_names]\n",
    "\n",
    "    predictions_df = pd.DataFrame({'image_name': test_file_names, 'PCI': flattened_predictions})\n",
    "    predictions_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "    process_csv_to_json(output_csv_file, epoch, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.9.1]",
   "language": "python",
   "name": "conda-env-opence-v1.9.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
